# Grivredr - System Architecture

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INTERFACES                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Web    â”‚  â”‚ WhatsApp â”‚  â”‚   API    â”‚  â”‚  Python  â”‚       â”‚
â”‚  â”‚Interface â”‚  â”‚   Bot    â”‚  â”‚  Client  â”‚  â”‚  Script  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FASTAPI REST API (main.py)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ /api/learn    â”‚  â”‚ /api/submit   â”‚  â”‚ /api/status   â”‚      â”‚
â”‚  â”‚ /api/scrapers â”‚  â”‚ /api/batch    â”‚  â”‚ /api/munic.   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                  â”‚                  â”‚
           â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Website Learner  â”‚  â”‚ Scraper Executor â”‚  â”‚ Status Checker   â”‚
â”‚  (learner.py)    â”‚  â”‚  (runner.py)     â”‚  â”‚  (runner.py)     â”‚
â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚
â”‚ â€¢ Playwright     â”‚  â”‚ â€¢ Load scraper   â”‚  â”‚ â€¢ Run status     â”‚
â”‚ â€¢ Browser auto   â”‚  â”‚ â€¢ Execute        â”‚  â”‚   scraper        â”‚
â”‚ â€¢ Screenshots    â”‚  â”‚ â€¢ Handle retry   â”‚  â”‚ â€¢ Parse status   â”‚
â”‚ â€¢ HTML extract   â”‚  â”‚ â€¢ Save results   â”‚  â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚
         â–¼                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  Claude Vision   â”‚           â”‚
â”‚   AI Analysis    â”‚           â”‚
â”‚  (ai_client.py)  â”‚           â”‚
â”‚                  â”‚           â”‚
â”‚ â€¢ Analyze form   â”‚           â”‚
â”‚ â€¢ Identify fieldsâ”‚           â”‚
â”‚ â€¢ Return JSON    â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
         â”‚                     â”‚
         â–¼                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚ Scraper Generatorâ”‚           â”‚
â”‚  (generator.py)  â”‚           â”‚
â”‚                  â”‚           â”‚
â”‚ â€¢ Call Claude    â”‚           â”‚
â”‚ â€¢ Generate code  â”‚           â”‚
â”‚ â€¢ Save to file   â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
         â”‚                     â”‚
         â–¼                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Generated Scrapers (Python Files)                â”‚
â”‚  generated_scrapers/                                          â”‚
â”‚    â”œâ”€â”€ ranchi/                                                â”‚
â”‚    â”‚   â”œâ”€â”€ ranchi_complaint_form_scraper.py                   â”‚
â”‚    â”‚   â”œâ”€â”€ ranchi_status_checker_scraper.py                   â”‚
â”‚    â”‚   â””â”€â”€ __init__.py                                        â”‚
â”‚    â”œâ”€â”€ patna/                                                 â”‚
â”‚    â””â”€â”€ ...                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â–²
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           (Executor loads and runs scrapers)
```

## ğŸ”„ Data Flow

### Flow 1: Learning & Generation (One-time)

```
Municipality URL
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WebsiteLearner   â”‚ â”€â”€â–º Open browser (Playwright)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Navigate & Click â”‚ â”€â”€â–º Find grievance form
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Take Screenshot  â”‚ â”€â”€â–º Full page PNG + HTML
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude Vision    â”‚ â”€â”€â–º Analyze structure
â”‚ (Sonnet 4.5)     â”‚     Return JSON
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
{
  "form_fields": [
    {"label": "Name", "selector": "#name", ...},
    {"label": "Phone", "selector": "#phone", ...}
  ],
  "submit_button": {"selector": "#submit"},
  ...
}
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude Opus      â”‚ â”€â”€â–º Generate Python code
â”‚ (Code Gen)       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
class RanchiScraper:
    async def submit_grievance(self, data):
        # AI-generated Playwright automation
        ...
        return {"success": True, "tracking_id": "..."}
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save to File     â”‚ â”€â”€â–º generated_scrapers/ranchi/...
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flow 2: Grievance Submission (Repeated, Fast)

```
User Grievance Data
{
  "name": "John",
  "complaint": "...",
  ...
}
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastAPI Endpoint â”‚ â”€â”€â–º /api/submit
â”‚ (main.py)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ScraperExecutor  â”‚ â”€â”€â–º Load generated scraper
â”‚ (runner.py)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dynamic Import   â”‚ â”€â”€â–º Import RanchiScraper class
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execute Scraper  â”‚ â”€â”€â–º scraper.submit_grievance(data)
â”‚ (No AI calls!)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Playwright       â”‚ â”€â”€â–º Open browser
â”‚ Automation       â”‚     Fill form
â”‚                  â”‚     Submit
â”‚                  â”‚     Capture tracking ID
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
{
  "success": true,
  "tracking_id": "RMC123",
  "screenshots": [...],
  "execution_time": 5.2
}
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save Result      â”‚ â”€â”€â–º executor/results/ranchi_...json
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Return to User   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Module Breakdown

### 1. config/ai_client.py
**Purpose:** Unified Claude API client

**Key Functions:**
- `analyze_website_structure()` - Vision analysis
- `generate_scraper_code()` - Code generation
- `improve_scraper_with_feedback()` - Self-healing
- `extract_status_from_page()` - Status parsing

**Models Used:**
- Haiku (fast/cheap)
- Sonnet (balanced)
- Opus (powerful)

### 2. website_learner/learner.py
**Purpose:** AI-powered website exploration

**Key Class:** `WebsiteLearner`

**Methods:**
- `learn_website()` - Explore single site
- `learn_multiple_websites()` - Batch learning
- `take_screenshot()` - Capture page
- `extract_form_html()` - Get form structure

**Output:** JSON with analysis + screenshots

### 3. scraper_generator/generator.py
**Purpose:** Convert analysis â†’ Python code

**Key Class:** `ScraperGenerator`

**Methods:**
- `generate_scraper()` - Create scraper file
- `refine_scraper_with_feedback()` - Fix errors
- `generate_scrapers_for_municipality()` - Batch gen

**Output:** Python files in `generated_scrapers/`

### 4. executor/runner.py
**Purpose:** Run generated scrapers

**Key Class:** `ScraperExecutor`

**Methods:**
- `execute_scraper()` - Run single scraper
- `execute_batch()` - Parallel execution
- `check_grievance_status()` - Status lookup
- `list_available_scrapers()` - Inventory

**Output:** Submission results + tracking IDs

### 5. main.py
**Purpose:** FastAPI REST API

**Endpoints:**
- `POST /api/learn` - Learn new municipality
- `POST /api/submit` - Submit grievance
- `POST /api/submit/batch` - Batch submit
- `POST /api/status` - Check status
- `GET /api/scrapers` - List scrapers
- `GET /api/municipalities` - List configs

## ğŸ—„ï¸ Data Storage

### File System Structure

```
grivredr/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ municipalities.json          # Municipality configurations
â”‚
â”œâ”€â”€ website_learner/
â”‚   â”œâ”€â”€ screenshots/                 # Learning screenshots
â”‚   â”‚   â”œâ”€â”€ ranchi_main_portal.png
â”‚   â”‚   â””â”€â”€ ranchi_complaint_form.png
â”‚   â””â”€â”€ results_ranchi.json          # Raw learning results
â”‚
â”œâ”€â”€ generated_scrapers/              # AI-generated code
â”‚   â”œâ”€â”€ ranchi/
â”‚   â”‚   â”œâ”€â”€ ranchi_complaint_form_scraper.py
â”‚   â”‚   â”œâ”€â”€ ranchi_status_checker_scraper.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ patna/
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ executor/
    â””â”€â”€ results/                     # Execution results
        â”œâ”€â”€ ranchi_complaint_form_20231223_120045.json
        â””â”€â”€ ranchi_status_check_20231223_120145.json
```

### municipalities.json Schema

```json
{
  "ranchi": {
    "name": "Ranchi Municipal Corporation",
    "websites": [
      {
        "url": "https://...",
        "type": "complaint_form",
        "description": "..."
      }
    ],
    "generated_scrapers": [
      {
        "file_path": "generated_scrapers/ranchi/...",
        "metadata": {
          "generated_at": "2023-12-23T12:00:00",
          "url": "...",
          "website_type": "complaint_form"
        }
      }
    ],
    "last_updated": "2023-12-23T12:00:00"
  }
}
```

### Execution Result Schema

```json
{
  "timestamp": "20231223_120045",
  "municipality": "ranchi",
  "website_type": "complaint_form",
  "input_data": {
    "name": "John Doe",
    "complaint": "..."
  },
  "result": {
    "success": true,
    "tracking_id": "RMC123",
    "screenshots": ["path/to/screenshot.png"],
    "execution_time": 5.2,
    "attempts": 1
  }
}
```

## ğŸ” Security Architecture

### API Key Management
- Stored in `.env` (gitignored)
- Loaded via `python-dotenv`
- Used only by `ai_client.py`

### Input Validation
- Pydantic models validate all API inputs
- Municipality names sanitized
- File paths checked for directory traversal

### Browser Security
- Playwright runs in isolated context
- No persistent cookies/storage
- Each execution starts fresh

### Rate Limiting (Recommended)
```python
from slowapi import Limiter

@app.post("/api/submit")
@limiter.limit("10/minute")
async def submit_grievance(...):
    ...
```

## âš¡ Performance Characteristics

### Learning Phase
- **Time:** 30-120 seconds per website
- **Cost:** ~$0.12 per website (AI costs)
- **Frequency:** One-time (or when site changes)

### Execution Phase
- **Time:** 5-15 seconds per submission
- **Cost:** $0.00 (no AI calls)
- **Frequency:** Every submission

### Scalability
- **Concurrency:** Limited by browser instances
- **Bottleneck:** Playwright automation (CPU-bound)
- **Solution:** Horizontal scaling with multiple workers

## ğŸ”„ Error Handling & Resilience

### Retry Logic
```python
for attempt in range(max_retries + 1):
    try:
        result = await scraper.submit_grievance(data)
        if result['success']:
            return result
        await asyncio.sleep(2)  # Wait before retry
    except Exception as e:
        if attempt < max_retries:
            continue
        return error_result
```

### Self-Healing
When scraper fails:
1. Capture error log
2. Take screenshot at failure point
3. Send to Claude with original code
4. Generate improved version
5. Save as new version (backup old)

### Failure Modes
| Failure | Cause | Recovery |
|---------|-------|----------|
| Selector not found | Website changed | Re-learn site |
| Timeout | Slow site | Increase timeout |
| Submit failed | Form validation | Check data format |
| AI API error | Network/quota | Retry with backoff |
| Import error | Scraper not found | Run learning |

## ğŸš€ Deployment Options

### Option 1: Single Server
```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

### Option 2: Docker
```dockerfile
FROM python:3.11
RUN playwright install chromium --with-deps
CMD ["uvicorn", "main:app"]
```

### Option 3: Kubernetes
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grivredr
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: grivredr
        image: grivredr:latest
        env:
        - name: api_key
          valueFrom:
            secretKeyRef:
              name: grivredr-secrets
              key: api-key
```

## ğŸ“Š Monitoring & Observability

### Metrics to Track
- Submissions per municipality
- Success rate by municipality
- Average execution time
- AI API costs
- Scraper failure rate

### Logging
```python
import logging

logger.info(f"Submitting to {municipality}")
logger.warning(f"Retry attempt {attempt}")
logger.error(f"Submission failed: {error}")
```

### Health Checks
```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "scrapers_count": len(list_scrapers()),
        "ai_client": "connected"
    }
```

## ğŸ”® Future Enhancements

### 1. Auto-Healing Scrapers
- Monitor success rates
- Auto-regenerate on repeated failures
- A/B test scraper versions

### 2. Scraper Versioning
- Git-based version control
- Rollback to previous versions
- Track performance over time

### 3. Multi-Model Support
- Try GPT-4V for comparison
- Fallback between models
- Cost optimization

### 4. Browser Pool
- Pre-launch browser instances
- Reuse connections
- Faster execution

### 5. Database Integration
- PostgreSQL for submissions
- Redis for caching
- Elasticsearch for search

---

**This architecture prioritizes:**
- âœ… Cost efficiency (AI only during learning)
- âœ… Scalability (generated code runs without AI)
- âœ… Resilience (retries, self-healing)
- âœ… Maintainability (clear separation of concerns)
