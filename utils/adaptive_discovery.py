"""
AI-Driven Adaptive Discovery Module

This module implements the Darshi vision: NO HARDCODED PATTERNS.
Everything is discovered dynamically using AI Vision.

Supports multiple AI providers through multi_provider_client:
- Anthropic (Claude)
- OpenAI (GPT-4 Vision)
- Google Gemini
- OpenRouter

Key principles:
1. AI looks at screenshots to detect success/error states
2. Timing is measured from actual behavior, not defaults
3. Frameworks are detected by testing behavior, not matching class names
4. Test data is generated by AI reading the form labels
"""

import asyncio
import base64
import json
import logging
import re
import time
from typing import Dict, Any, List, Optional, Tuple
from playwright.async_api import Page

logger = logging.getLogger(__name__)


class AIDiscovery:
    """
    Pure AI-driven discovery - zero hardcoded patterns.
    
    Works with any AI provider (Anthropic, OpenAI, Gemini, OpenRouter)
    through the MultiProviderAIClient.
    """
    
    def __init__(self, ai_client=None):
        """
        Initialize AI Discovery module.
        
        Args:
            ai_client: The MultiProviderAIClient for making vision API calls.
                      If None, will import from config on first use.
        """
        self._ai_client = ai_client
    
    @property
    def ai_client(self):
        """Lazy load AI client if not provided."""
        if self._ai_client is None:
            try:
                from config.multi_provider_client import ai_client
                self._ai_client = ai_client
            except ImportError:
                logger.warning("Could not import AI client, some features will be limited")
        return self._ai_client
    
    def _build_vision_message(self, screenshot_b64: str, prompt: str) -> List[Dict[str, Any]]:
        """
        Build a vision message that works with multiple providers.
        
        OpenAI/Gemini format is slightly different from Anthropic, but the 
        multi_provider_client handles translation.
        """
        return [{
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/png",
                        "data": screenshot_b64
                    }
                },
                {"type": "text", "text": prompt}
            ]
        }]
    
    def _parse_json_response(self, response_text: str) -> Dict[str, Any]:
        """Extract JSON from AI response text."""
        try:
            # Try to find JSON block in response
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                return json.loads(json_match.group())
        except json.JSONDecodeError:
            pass
        return {}
    
    async def detect_success_or_error(self, page: Page) -> Dict[str, Any]:
        """
        Use AI Vision to determine if form submission succeeded.
        
        Works with any configured AI provider (Claude, GPT-4, Gemini, etc.)
        NO KEYWORD MATCHING - purely visual analysis.
        
        Returns:
            Dict with:
                - submission_status: "success" | "error" | "unknown"
                - confidence: 0.0-1.0
                - tracking_id: extracted tracking/reference ID or None
                - errors: list of error messages detected
                - visual_evidence: what AI saw that led to conclusion
        """
        if not self.ai_client:
            logger.warning("No AI client available for vision detection")
            return {"submission_status": "unknown", "confidence": 0.0}
        
        try:
            from config.settings import TaskType
            
            # Take screenshot
            screenshot = await page.screenshot()
            screenshot_b64 = base64.b64encode(screenshot).decode()
            
            prompt = """Analyze this form page screenshot. Answer these questions:

1. SUBMISSION STATUS: Did a form submission succeed? Look for:
   - Green checkmarks, success icons, confirmation banners
   - Messages containing "success", "धन्यवाद", "submitted", "registered"
   - Tracking/reference numbers displayed prominently
   - Thank you pages or confirmation screens
   - OR: Red text, error icons, warning messages, validation failures

2. TRACKING ID: If successful, is there a tracking/reference/complaint number visible?
   Extract the EXACT alphanumeric ID shown.

3. ERRORS: If there are errors, what specific error messages are shown?
   List each error exactly as displayed.

Respond ONLY with valid JSON:
{
    "submission_status": "success" | "error" | "unknown",
    "confidence": 0.0-1.0,
    "tracking_id": "EXACT_ID_HERE" or null,
    "errors": ["error message 1", "error message 2"] or [],
    "visual_evidence": "brief description of what you saw"
}"""

            # Use multi-provider client with TaskType for vision
            messages = self._build_vision_message(screenshot_b64, prompt)
            response = self.ai_client.create_message(
                messages=messages,
                task_type=TaskType.VISION,
            )
            
            # Extract text from response (format varies by provider)
            response_text = response.get("content", "")
            if isinstance(response_text, list):
                response_text = response_text[0].get("text", "") if response_text else ""
            
            result = self._parse_json_response(response_text)
            
            if not result:
                logger.warning("Could not parse AI vision response")
                return {"submission_status": "unknown", "confidence": 0.0, "raw_response": response_text}
            
            logger.info(f"AI Vision detected: {result.get('submission_status')} (confidence: {result.get('confidence', 0)})")
            return result
            
        except Exception as e:
            logger.error(f"AI vision detection failed: {e}")
            return {"submission_status": "unknown", "confidence": 0.0, "error": str(e)}
    
    async def measure_cascade_timing(
        self,
        page: Page,
        parent_selector: str,
        child_selector: str
    ) -> float:
        """
        Measure actual cascade load time by observing DOM changes.
        
        NO DEFAULT VALUES - we measure what actually happens.
        
        Args:
            page: Playwright page
            parent_selector: CSS selector for parent dropdown
            child_selector: CSS selector for child dropdown
            
        Returns:
            Measured timing in seconds (with 20% buffer), or 0.0 if no cascade detected
        """
        try:
            # Get initial child option count
            initial_count = await page.evaluate(f"""
                () => {{
                    const child = document.querySelector('{child_selector}');
                    if (!child) return 0;
                    if (child.tagName === 'SELECT') return child.options.length;
                    // For custom dropdowns, count visible items
                    return child.querySelectorAll('[role="option"], .option, li').length;
                }}
            """)
            
            # Select first real option in parent (index 1, skipping placeholder)
            try:
                await page.select_option(parent_selector, index=1, timeout=3000)
            except Exception:
                # Maybe not a native select, try clicking
                await page.click(parent_selector)
                await asyncio.sleep(0.3)
                # Click first option
                options = page.locator(f"{parent_selector} option, .select2-results__option, .ant-select-item")
                if await options.count() > 1:
                    await options.nth(1).click()
            
            # Poll for change with tight timing
            start = time.time()
            max_wait = 10.0  # Hard cap at 10 seconds
            
            while time.time() - start < max_wait:
                current_count = await page.evaluate(f"""
                    () => {{
                        const child = document.querySelector('{child_selector}');
                        if (!child) return 0;
                        if (child.tagName === 'SELECT') return child.options.length;
                        return child.querySelectorAll('[role="option"], .option, li').length;
                    }}
                """)
                
                if current_count > initial_count:
                    measured = time.time() - start
                    # Return measured time + 20% buffer
                    timing = measured * 1.2
                    logger.info(f"Measured cascade timing: {measured:.2f}s (using {timing:.2f}s with buffer)")
                    return timing
                
                await asyncio.sleep(0.05)  # Poll every 50ms
            
            # If nothing changed in 10s, it's probably not a cascade
            logger.info("No cascade detected within 10s timeout")
            return 0.0
            
        except Exception as e:
            logger.warning(f"Cascade timing measurement failed: {e}")
            return 0.0
    
    async def detect_dropdown_type_by_behavior(self, page: Page, selector: str) -> str:
        """
        Detect dropdown type by TESTING behavior, not matching class names.
        
        NO CLASS NAME CHECKS - purely behavioral testing.
        
        Returns:
            "plain_html" | "select2" | "ant_design" | "custom" | "unknown"
        """
        try:
            # Test 1: Is it a native <select>?
            element_info = await page.evaluate(f"""
                (sel) => {{
                    const el = document.querySelector(sel);
                    if (!el) return {{ found: false }};
                    return {{
                        found: true,
                        tagName: el.tagName,
                        hasOptions: el.tagName === 'SELECT' && el.options.length > 0
                    }};
                }}
            """, selector)
            
            if not element_info.get("found"):
                return "unknown"
            
            if element_info.get("tagName") == "SELECT" and element_info.get("hasOptions"):
                # It's a native select - check if enhanced by JS
                # Count portal elements before click
                initial_portals = await page.evaluate("""
                    () => document.querySelectorAll(
                        '.select2-dropdown, .ant-select-dropdown, .p-dropdown-panel, .k-animation-container'
                    ).length
                """)
                
                # Click to potentially trigger enhancement
                await page.click(selector, timeout=2000)
                await asyncio.sleep(0.3)
                
                # Check for new portal elements
                new_portals = await page.evaluate("""
                    () => document.querySelectorAll(
                        '.select2-dropdown, .ant-select-dropdown, .p-dropdown-panel, .k-animation-container'
                    ).length
                """)
                
                # Close dropdown
                await page.keyboard.press("Escape")
                
                if new_portals > initial_portals:
                    # Enhanced select - now determine which library
                    portal_type = await page.evaluate("""
                        () => {
                            if (document.querySelector('.select2-dropdown')) return 'select2';
                            if (document.querySelector('.ant-select-dropdown')) return 'ant_design';
                            if (document.querySelector('.p-dropdown-panel')) return 'primeng';
                            if (document.querySelector('.k-animation-container')) return 'kendo';
                            return 'custom';
                        }
                    """)
                    logger.info(f"Detected enhanced dropdown: {portal_type}")
                    return portal_type
                
                logger.info("Detected plain HTML select")
                return "plain_html"
            
            # Not a native select - it's some custom dropdown
            # Try clicking and see what appears
            await page.click(selector, timeout=2000)
            await asyncio.sleep(0.5)
            
            # Check what appeared
            portal_type = await page.evaluate("""
                () => {
                    if (document.querySelector('.select2-dropdown')) return 'select2';
                    if (document.querySelector('.ant-select-dropdown')) return 'ant_design';
                    if (document.querySelector('.p-dropdown-panel')) return 'primeng';
                    if (document.querySelector('[role="listbox"]')) return 'custom';
                    return 'unknown';
                }
            """)
            
            await page.keyboard.press("Escape")
            logger.info(f"Detected dropdown type by behavior: {portal_type}")
            return portal_type
            
        except Exception as e:
            logger.warning(f"Dropdown type detection failed: {e}")
            return "unknown"
    
    async def generate_test_data_with_ai(self, page: Page, fields: List[Dict]) -> Dict[str, Any]:
        """
        Use AI to generate appropriate test data by looking at the form.
        
        Works with any configured AI provider (Claude, GPT-4, Gemini, etc.)
        NO FIELD NAME PATTERNS - AI reads labels and decides.
        
        Args:
            page: Playwright page with the form visible
            fields: List of field dicts with 'name', 'label', 'type' keys
            
        Returns:
            Dict mapping field_name to appropriate test value
        """
        if not self.ai_client:
            logger.warning("No AI client, using basic test data")
            return self._fallback_test_data(fields)
        
        try:
            from config.settings import TaskType
            
            # Take screenshot of the form
            screenshot = await page.screenshot()
            screenshot_b64 = base64.b64encode(screenshot).decode()
            
            # Build field descriptions
            fields_desc = "\n".join([
                f"- {f.get('label') or f.get('name')}: type={f.get('type')}, required={f.get('required', False)}"
                for f in fields
            ])
            
            prompt = f"""Look at this form and generate realistic test data for each field.

FIELDS TO FILL:
{fields_desc}

For each field, provide appropriate test data based on:
1. The field label you can see (in any language - Hindi, English, etc.)
2. Visual input constraints (length limits, format hints)
3. Placeholder text if visible
4. The type of form (this appears to be a grievance/complaint form)

Use realistic INDIAN data:
- Indian phone numbers (10 digits starting with 6-9)
- Indian addresses and pincodes
- Common Indian names
- Valid email formats

Respond ONLY with valid JSON mapping field names to test values:
{{"field_name_1": "appropriate value", "field_name_2": "value", ...}}

Use the exact field names from the list above."""

            messages = self._build_vision_message(screenshot_b64, prompt)
            response = self.ai_client.create_message(
                messages=messages,
                task_type=TaskType.VISION,
            )
            
            response_text = response.get("content", "")
            if isinstance(response_text, list):
                response_text = response_text[0].get("text", "") if response_text else ""
            
            result = self._parse_json_response(response_text)
            
            if result:
                logger.info(f"AI generated test data for {len(result)} fields")
                return result
            
            logger.warning("Could not parse AI test data response")
            return self._fallback_test_data(fields)
            
        except Exception as e:
            logger.error(f"AI test data generation failed: {e}")
            return self._fallback_test_data(fields)
    
    def _fallback_test_data(self, fields: List[Dict]) -> Dict[str, Any]:
        """
        Minimal fallback when AI is not available.
        Uses very generic defaults - NOT pattern matching.
        """
        data = {}
        for field in fields:
            name = field.get("name", "")
            field_type = field.get("type", "text")
            
            if field_type == "dropdown":
                # For dropdowns, we'll select the first option dynamically
                data[name] = "__FIRST_OPTION__"
            elif field_type == "textarea":
                data[name] = "Test complaint for automated testing purposes."
            else:
                data[name] = "Test Value"
        return data
    
    async def find_submit_button_with_ai(self, page: Page) -> Optional[str]:
        """
        Use AI vision to find the submit button.
        
        NO SELECTOR LISTS - AI looks at the page and identifies the button.
        
        Returns:
            Selector string for the submit button, or None if not found
        """
        if not self.ai_client:
            logger.warning("No AI client for submit button detection")
            return None
        
        try:
            from config.settings import TaskType
            
            screenshot = await page.screenshot()
            screenshot_b64 = base64.b64encode(screenshot).decode()
            
            prompt = """Find the form submit button on this page.

Look for buttons that:
- Are labeled Submit, Register, Send, Save, Continue, Next, Proceed
- Or equivalent in Hindi: प्रस्तुत करें, दर्ज करें, भेजें, सहेजें, अगला
- Are usually at the bottom of the form
- Are often styled prominently (blue, green, primary color)
- Have type="submit" attribute

Respond ONLY with valid JSON:
{
    "found": true/false,
    "button_text": "exact visible text on button",
    "button_description": "brief description of button appearance/location",
    "confidence": 0.0-1.0
}"""

            messages = self._build_vision_message(screenshot_b64, prompt)
            response = self.ai_client.create_message(
                messages=messages,
                task_type=TaskType.VISION,
            )
            
            response_text = response.get("content", "")
            if isinstance(response_text, list):
                response_text = response_text[0].get("text", "") if response_text else ""
            
            result = self._parse_json_response(response_text)
            
            if result.get("found") and result.get("button_text"):
                button_text = result["button_text"]
                # Construct selector from the text AI found
                selector = f'button:has-text("{button_text}"), input[value="{button_text}"]'
                logger.info(f"AI found submit button with text: '{button_text}'")
                return selector
            
            logger.warning("AI could not find submit button")
            return None
            
        except Exception as e:
            logger.error(f"AI submit button detection failed: {e}")
            return None
    
    async def analyze_form_structure(self, page: Page) -> Dict[str, Any]:
        """
        Use AI to understand the overall form structure and relationships.
        
        Works with any configured AI provider (Claude, GPT-4, Gemini, etc.)
        
        Returns insights about:
        - Field dependencies (cascading relationships)
        - Required vs optional fields
        - Field groupings
        - Suggested fill order
        """
        if not self.ai_client:
            return {}
        
        try:
            from config.settings import TaskType
            
            screenshot = await page.screenshot(full_page=True)
            screenshot_b64 = base64.b64encode(screenshot).decode()
            
            prompt = """Analyze this form's structure and provide insights:

1. FIELD DEPENDENCIES: Which fields depend on others?
   (e.g., "Sub-Category" depends on "Category")

2. REQUIRED FIELDS: Which fields appear to be required?
   (Look for asterisks, red labels, "required" text)

3. FIELD GROUPINGS: How are fields logically grouped?
   (e.g., "Personal Details", "Problem Details", "Location")

4. SUGGESTED ORDER: What order should fields be filled in?
   (Consider dependencies - parents before children)

5. SPECIAL HANDLING: Any fields that might need special interaction?
   (e.g., date pickers, file uploads, captcha)

Respond with JSON:
{
    "dependencies": [{"child": "field_name", "parent": "field_name"}, ...],
    "required_fields": ["field1", "field2", ...],
    "field_groups": {"group_name": ["field1", "field2"], ...},
    "fill_order": ["field1", "field2", ...],
    "special_fields": [{"name": "field_name", "type": "datepicker|captcha|file", "notes": "..."}]
}"""

            messages = self._build_vision_message(screenshot_b64, prompt)
            response = self.ai_client.create_message(
                messages=messages,
                task_type=TaskType.VISION,
            )
            
            response_text = response.get("content", "")
            if isinstance(response_text, list):
                response_text = response_text[0].get("text", "") if response_text else ""
            
            result = self._parse_json_response(response_text)
            
            if result:
                logger.info(f"AI analyzed form structure: {len(result.get('dependencies', []))} dependencies found")
                return result
            
            return {}
            
        except Exception as e:
            logger.error(f"Form structure analysis failed: {e}")
            return {}


# Global instance for convenience (lazy initialized with AI client)
ai_discovery = AIDiscovery()
