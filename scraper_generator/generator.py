"""
Scraper Generator - Generates reusable scraper code from website analysis
"""
import json
import logging
import re
from pathlib import Path
from typing import Dict, Any
from config.ai_client import ai_client

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ScraperGenerator:
    """
    Generates production-ready scraper code from AI website analysis
    """

    def __init__(self, output_dir: str = "generated_scrapers"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

    def generate_scraper(
        self,
        website_analysis: Dict[str, Any],
        municipality_name: str,
        website_type: str
    ) -> Dict[str, Any]:
        """
        Generate scraper code from website analysis

        Args:
            website_analysis: Result from WebsiteLearner
            municipality_name: e.g., "ranchi"
            website_type: e.g., "complaint_form", "status_checker"

        Returns:
            {
                "success": bool,
                "file_path": str,
                "code": str,
                "metadata": dict
            }
        """
        logger.info(f"Generating scraper for {municipality_name} - {website_type}")

        if not website_analysis.get("success"):
            return {
                "success": False,
                "error": "Website analysis failed",
                "analysis": website_analysis
            }

        try:
            # Generate code using AI
            url = website_analysis["url"]
            analysis_str = json.dumps(website_analysis["analysis"], indent=2)

            logger.info("Requesting AI to generate scraper code")
            raw_code = ai_client.generate_scraper_code(
                website_analysis=analysis_str,
                url=url,
                municipality_name=municipality_name
            )

            # Clean up the code (remove markdown formatting)
            clean_code = self._extract_python_code(raw_code)

            # Add metadata header
            code_with_header = self._add_code_header(
                clean_code,
                municipality_name,
                website_type,
                url,
                website_analysis
            )

            # Save to file
            file_name = f"{municipality_name}_{website_type}_scraper.py"
            file_path = self.output_dir / municipality_name / file_name

            # Create municipality directory
            (self.output_dir / municipality_name).mkdir(exist_ok=True)

            # Write file
            file_path.write_text(code_with_header)

            logger.info(f"Scraper saved to: {file_path}")

            # Also create __init__.py for easy imports
            init_file = self.output_dir / municipality_name / "__init__.py"
            if not init_file.exists():
                init_file.write_text("# Generated scrapers\n")

            return {
                "success": True,
                "file_path": str(file_path),
                "code": code_with_header,
                "metadata": {
                    "municipality": municipality_name,
                    "website_type": website_type,
                    "url": url,
                    "generated_at": str(Path(file_path).stat().st_mtime)
                }
            }

        except Exception as e:
            logger.error(f"Failed to generate scraper: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    def _extract_python_code(self, response: str) -> str:
        """Extract Python code from AI response"""
        # Look for ```python ... ``` blocks
        python_match = re.search(r'```python\s*(.*?)\s*```', response, re.DOTALL)
        if python_match:
            return python_match.group(1).strip()

        # Look for ``` ... ``` blocks
        code_match = re.search(r'```\s*(.*?)\s*```', response, re.DOTALL)
        if code_match:
            return code_match.group(1).strip()

        # If no code blocks, assume entire response is code
        return response.strip()

    def _add_code_header(
        self,
        code: str,
        municipality_name: str,
        website_type: str,
        url: str,
        analysis: Dict[str, Any]
    ) -> str:
        """Add metadata header to generated code"""
        header = f'''"""
Auto-generated scraper for {municipality_name.title()} - {website_type}
URL: {url}
Generated: {Path().ctime() if hasattr(Path(), 'ctime') else 'N/A'}

This scraper was automatically generated by AI based on website analysis.
It may require adjustments if the website structure changes.

Analysis Summary:
{json.dumps(analysis.get("metadata", {}), indent=2)}
"""

'''
        return header + code

    def refine_scraper_with_feedback(
        self,
        scraper_path: str,
        error_log: str,
        screenshot_base64: str = None
    ) -> Dict[str, Any]:
        """
        Improve scraper based on execution errors

        Args:
            scraper_path: Path to the failing scraper
            error_log: Error message/traceback
            screenshot_base64: Optional screenshot of failure

        Returns:
            Updated scraper info
        """
        logger.info(f"Refining scraper: {scraper_path}")

        try:
            # Read current code
            original_code = Path(scraper_path).read_text()

            # Get improved code from AI
            improved_code = ai_client.improve_scraper_with_feedback(
                original_code=original_code,
                error_log=error_log,
                screenshot_base64=screenshot_base64
            )

            # Clean up
            clean_code = self._extract_python_code(improved_code)

            # Backup original
            backup_path = Path(scraper_path).with_suffix('.py.bak')
            Path(scraper_path).rename(backup_path)

            # Write improved version
            Path(scraper_path).write_text(clean_code)

            logger.info(f"Scraper refined and saved. Backup: {backup_path}")

            return {
                "success": True,
                "file_path": scraper_path,
                "backup_path": str(backup_path),
                "code": clean_code
            }

        except Exception as e:
            logger.error(f"Failed to refine scraper: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    def generate_scrapers_for_municipality(
        self,
        learning_results: list[Dict[str, Any]],
        municipality_name: str
    ) -> Dict[str, Any]:
        """
        Generate all scrapers for a municipality

        Args:
            learning_results: List of website learning results
            municipality_name: Municipality name

        Returns:
            Summary of generated scrapers
        """
        logger.info(f"Generating all scrapers for {municipality_name}")

        generated = []
        failed = []

        for result in learning_results:
            if not result.get("success"):
                failed.append({
                    "url": result.get("url"),
                    "error": result.get("error")
                })
                continue

            scraper_result = self.generate_scraper(
                website_analysis=result,
                municipality_name=municipality_name,
                website_type=result.get("website_type", "unknown")
            )

            if scraper_result.get("success"):
                generated.append(scraper_result)
            else:
                failed.append({
                    "url": result.get("url"),
                    "error": scraper_result.get("error")
                })

        return {
            "municipality": municipality_name,
            "generated_count": len(generated),
            "failed_count": len(failed),
            "generated": generated,
            "failed": failed
        }


def test_generator():
    """Test scraper generation"""
    # Mock website analysis
    mock_analysis = {
        "success": True,
        "url": "https://example.com/complaint",
        "analysis": {
            "form_fields": [
                {
                    "label": "Name",
                    "type": "text",
                    "selector": "#name",
                    "required": True
                },
                {
                    "label": "Complaint",
                    "type": "textarea",
                    "selector": "#complaint",
                    "required": True
                }
            ],
            "submit_button": {
                "selector": "#submit",
                "text": "Submit"
            }
        },
        "metadata": {
            "municipality": "test",
            "page_title": "Test Portal"
        }
    }

    generator = ScraperGenerator()
    result = generator.generate_scraper(
        website_analysis=mock_analysis,
        municipality_name="test",
        website_type="complaint_form"
    )

    print(json.dumps(result, indent=2, default=str))


if __name__ == "__main__":
    test_generator()
